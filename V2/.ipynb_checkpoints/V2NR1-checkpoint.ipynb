{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-b413f7d180fb>, line 46)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-b413f7d180fb>\"\u001b[0;36m, line \u001b[0;32m46\u001b[0m\n\u001b[0;31m    T_test=\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "# a).1\n",
    "# fun_true(X): fun_true(X) bekommt einen Spaltenvektor Ã¼bergeben.\n",
    "    # Sie initialisiert die Parameter w0,w1, und w2 von y(x). Sie berechnet y(x) und gibt es zurÃ¼ck.\n",
    "# generateDataSet(N, xmin, xmax, sd_noise): generateDataSet(.) berechnet den Datenvektor X.\n",
    "    # Um y(x) zu berechnen, wird fun_true aufgerufen. Falls sd_noise positiv ist, \n",
    "    # wird T mit zufÃ¤lligen Werten innerhalb der Standardabweichung kumuliert. generateDataSet(.) gibt X und T zurÃ¼ck.\n",
    "# getDataError(Y,T): Berechnet die Abweichung zwischen der Prognose Y und den erzielten Werten T. \n",
    "    # DafÃ¼r werden die Abweichungen der Matrizen multipliziert, sodass sie positiv werden. \n",
    "    # Die Ergebnisse werden aufsummiert und halbiert. Dies ergibt die Daten der kleinsten Quadrate (Matrix?).\n",
    "# phi_polynomial(x, deg=1): phi_polynomial(.) bekommt x und ein Grad uebergeben, der im Standardfall 1 entspricht.\n",
    "    # Sie prueft ob x noch ein Zeilen-Vektor ist. Sie gibt ein Array (Zeilenvektor) von x[0]**i zurÃ¼ck. \n",
    "    # Zum Schluss wird der Vektor transponiert, dass es ein Spaltenvektor ist.   \n",
    "    \n",
    "# a).2\n",
    "# Die Funktion fun_true(X): sampelt die Originaldaten (xn, tn), da aus den Werten y(x) berechnet und zurÃ¼ckgegeben wird.\n",
    "# Siehe Aufgabenbeschreibung \"die Werte wurden durch die Parabel f(x) = .. gesampelt.\"\n",
    "\n",
    "# a).3\n",
    "# phi(x) = [1, x, x**2, x**3, ..., x**deg], da Zufallswerte verwendet werden, kann das Ergebnis nicht von Hand bestimmt werden.\n",
    "\n",
    "# phi1 = [[ 1.00000000e+00  2.71320643e+00  7.36148915e+00  1.99732397e+01 5.41915225e+01  1.47032787e+02]\n",
    "# phi2 = [ 1.00000000e+00 -4.79248051e+00  2.29678694e+01 -1.10073066e+02 5.27523025e+02 -2.52814381e+03]\n",
    "# phi3 = [ 1.00000000e+00  1.33648235e+00  1.78618507e+00  2.38720482e+00 3.19045710e+00  4.26398961e+00]\n",
    "# phi4 = [ 1.00000000e+00  2.48803883e+00  6.19033720e+00  1.54017993e+01 3.83202746e+01  9.53423310e+01]\n",
    "# phi5 = [ 1.00000000e+00 -1.49298770e-02  2.22901226e-04 -3.32788789e-06 4.96849568e-08 -7.41790292e-10]\n",
    "# phi6 = [ 1.00000000e+00 -2.75203354e+00  7.57368863e+00 -2.08430452e+01 5.73607595e+01 -1.57858734e+02]\n",
    "# phi7 = [ 1.00000000e+00 -3.01937135e+00  9.11660336e+00 -2.75264110e+01 8.31124569e+01 -2.50947371e+02]\n",
    "# phi8 = [ 1.00000000e+00  2.60530712e+00  6.78762520e+00  1.76838483e+01 4.60718559e+01  1.20031334e+02]\n",
    "# phi9 = [ 1.00000000e+00 -3.30889163e+00  1.09487638e+01 -3.62282731e+01 1.19875430e+02 -3.96654807e+02]\n",
    "# phi10 = [ 1.00000000e+00 -4.11660186e+00  1.69464109e+01 -6.97616264e+01 2.87180841e+02 -1.18220918e+03]]\n",
    "\n",
    "# a).4\n",
    "# lambda regularisiert die Least-Squares. Da lambda im angegebenen Fall 0 ist fÃ¤llt der Teil (lambda/2)*wT*w weg.\n",
    "\n",
    "#a).5\n",
    "# X, T sind die Trainingsdaten, X_test, T_test sind die Testdaten. \n",
    "# Die Testdaten werden vorbehalten wÃ¤hrend des Trainierens, somit kann durch mehrmalige Krezvalidierung sÃ¤mtliche Daten zum \n",
    "# Testen verwendet werden. Im hier gezeigten Fall, werden beide Ã¼ber eine Random-Funktion gefÃ¼llt.\n",
    "\n",
    "T_test= \n",
    "[[ 3.10905545]     \n",
    " [57.97094574]\n",
    " [ 5.36688144]\n",
    " [15.48746047]\n",
    " [ 0.92351025]\n",
    " [-1.52698415]\n",
    " [ 6.31013154]\n",
    " [-2.84101855]\n",
    " [20.36655269]\n",
    " [ 6.00240429]]\n",
    "\n",
    "\n",
    "T= [[24.02637686]\n",
    " [76.78157398]\n",
    " [ 6.06498717]\n",
    " [16.33697066]\n",
    " [ 6.34586048]\n",
    " [39.50347318]\n",
    " [22.71852474]\n",
    " [30.04030926]\n",
    " [40.44148448]\n",
    " [61.40721056]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X= [[ 2.71320643]\n",
      " [-4.79248051]\n",
      " [ 1.33648235]\n",
      " [ 2.48803883]\n",
      " [-0.01492988]\n",
      " [-2.75203354]\n",
      " [-3.01937135]\n",
      " [ 2.60530712]\n",
      " [-3.30889163]\n",
      " [-4.11660186]] T= [[ 24.02637686]\n",
      " [ 76.78157398]\n",
      " [  6.06498717]\n",
      " [ 16.33697066]\n",
      " [  6.34586048]\n",
      " [ 39.50347318]\n",
      " [ 22.71852474]\n",
      " [ 30.04030926]\n",
      " [ 40.44148448]\n",
      " [ 61.40721056]]\n",
      "PHI= [[  1.00000000e+00   2.71320643e+00   7.36148915e+00   1.99732397e+01\n",
      "    5.41915225e+01   1.47032787e+02]\n",
      " [  1.00000000e+00  -4.79248051e+00   2.29678694e+01  -1.10073066e+02\n",
      "    5.27523025e+02  -2.52814381e+03]\n",
      " [  1.00000000e+00   1.33648235e+00   1.78618507e+00   2.38720482e+00\n",
      "    3.19045710e+00   4.26398961e+00]\n",
      " [  1.00000000e+00   2.48803883e+00   6.19033720e+00   1.54017993e+01\n",
      "    3.83202746e+01   9.53423310e+01]\n",
      " [  1.00000000e+00  -1.49298770e-02   2.22901226e-04  -3.32788789e-06\n",
      "    4.96849568e-08  -7.41790292e-10]\n",
      " [  1.00000000e+00  -2.75203354e+00   7.57368863e+00  -2.08430452e+01\n",
      "    5.73607595e+01  -1.57858734e+02]\n",
      " [  1.00000000e+00  -3.01937135e+00   9.11660336e+00  -2.75264110e+01\n",
      "    8.31124569e+01  -2.50947371e+02]\n",
      " [  1.00000000e+00   2.60530712e+00   6.78762520e+00   1.76838483e+01\n",
      "    4.60718559e+01   1.20031334e+02]\n",
      " [  1.00000000e+00  -3.30889163e+00   1.09487638e+01  -3.62282731e+01\n",
      "    1.19875430e+02  -3.96654807e+02]\n",
      " [  1.00000000e+00  -4.11660186e+00   1.69464109e+01  -6.97616264e+01\n",
      "    2.87180841e+02  -1.18220918e+03]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (6,6) and (10,6) not aligned: 6 (dim 1) != 10 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-623d0885ce5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m#W_LSR = np.zeros((M,1))                                           # REPLACE THIS BY REGULARIZED LEAST SQUARES WEIGHTS!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mW_LSR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPHI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPHI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlmbda\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPHI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPHI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (6,6) and (10,6) not aligned: 6 (dim 1) != 10 (dim 0)"
     ]
    }
   ],
   "source": [
    "# V2A1_LinearRegression.py \n",
    "# Programmgeruest zu Versuch 2, Aufgabe 1\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def fun_true(X):                              # compute 1-dim. parable function; X must be Nx1 data matrix\n",
    "    w2,w1,w0 = 3.0,-1.0,2.0                   # true parameters of parable y(x)=w0+w1*x+w2*x*x\n",
    "    return w0+w1*X+w2*np.multiply(X,X)        # return function values (same size as X)\n",
    "\n",
    "def generateDataSet(N,xmin,xmax,sd_noise):    # generate data matrix X and target values T\n",
    "    X=xmin+np.random.rand(N,1)*(xmax-xmin)    # get random x values uniformly in [xmin;xmax)\n",
    "    T=fun_true(X);                            # target values without noise\n",
    "    if(sd_noise>0):\n",
    "        T=T+np.random.normal(0,sd_noise,X.shape) # add noise \n",
    "    return X,T\n",
    "\n",
    "def getDataError(Y,T):                        # compute data error (least squares) between prediction Y and true target values T\n",
    "    D=np.multiply(Y-T,Y-T);                   # squared differences between Y and T\n",
    "    return 0.5*sum(sum(D));                   # return least-squares data error function E_D\n",
    "\n",
    "def phi_polynomial(x,deg=1):                            # compute polynomial basis function vector phi(x) for data x \n",
    "    assert(np.shape(x)==(1,)), \"currently only 1dim data supported\"\n",
    "    return np.array([x[0]**i for i in range(deg+1)]).T; # returns feature vector phi(x)=[1 x x**2 x**3 ... x**deg]\n",
    "\n",
    "# (I) generate data \n",
    "np.random.seed(10)                            # set seed of random generator (to be able to regenerate data)\n",
    "N=10                                          # number of data samples\n",
    "xmin,xmax=-5.0,5.0                            # x limits\n",
    "sd_noise=10                                   # standard deviation of Guassian noise\n",
    "X,T           = generateDataSet(N, xmin,xmax, sd_noise)             # generate training data\n",
    "X_test,T_test = generateDataSet(N, xmin,xmax, sd_noise)             # generate test data\n",
    "print (\"X=\",X, \"T=\",T)\n",
    "\n",
    "# (II) generate linear least squares model for regression\n",
    "lmbda=0                                                           # no regression\n",
    "deg=5                                                             # degree of polynomial basis functions\n",
    "N,D = np.shape(X)                                                 # shape of data matrix X\n",
    "N,K = np.shape(T)                                                 # shape of target value matrix T\n",
    "PHI = np.array([phi_polynomial(X[i],deg).T for i in range(N)])    # generate design matrix\n",
    "N,M = np.shape(PHI)                                               # shape of design matrix\n",
    "print (\"PHI=\", PHI)\n",
    "\n",
    "#W_LSR = np.zeros((M,1))                                           # REPLACE THIS BY REGULARIZED LEAST SQUARES WEIGHTS!  \n",
    "W_LSR = np.dot(np.linalg.inv(np.dot(np.dot(PHI.T,PHI),lmbda*np.ones(PHI.shape))),PHI.T*t)\n",
    "\n",
    "\n",
    "print (\"W_LSR=\",W_LSR)\n",
    "\n",
    "# (III) make predictions for test data\n",
    "Y_test = np.zeros((N,1))   # REPLACE THIS BY PROGNOSIS FOR TEST DATA X_test! (result should be N x 1 matrix, i.e., one prognosis per row) \n",
    "Y_learn = np.zeros((N,1))  # REPLACE THIS BY PROGNOSIS FOR TEST DATA X_test! (result should be N x 1 matrix, i.e., one prognosis per row)\n",
    "print (\"Y_test=\",Y_test)\n",
    "print (\"T_test=\",T_test)\n",
    "print (\"learn data error = \", getDataError(Y_learn,T))\n",
    "print (\"test data error = \", getDataError(Y_test,T_test))\n",
    "print (\"W_LSR=\",W_LSR)\n",
    "print (\"mean weight = \", np.mean(np.mean(np.abs(W_LSR))))\n",
    "\n",
    "# (IV) plot data\n",
    "ymin,ymax = -50.0,150.0                     # interval of y data\n",
    "x_=np.arange(xmin,xmax,0.01)                # densely sampled x values\n",
    "Y_LSR = np.array([np.dot(W_LSR.T,np.array([phi_polynomial([x],deg)]).T)[0] for x in x_]);   # least squares prediction\n",
    "Y_true = fun_true(x_).flat\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(X.flat,T.flat,c='g',marker='x',s=100)             # plot learning data points (green x)\n",
    "ax.scatter(X_test.flat,T_test.flat,c='g',marker='.',s=100)   # plot test data points (green .)\n",
    "ax.plot(x_,Y_LSR.flat, c='r')         # plot LSR regression curve (red)\n",
    "ax.plot(x_,Y_true, c='g')             # plot true function curve (green)\n",
    "ax.set_xlabel('x')                    # label on x-axis\n",
    "ax.set_ylabel('y')                    # label on y-axis\n",
    "ax.grid()                             # draw a grid\n",
    "plt.ylim((ymin,ymax))                 # set y-limits\n",
    "plt.show()                            # show plot on screen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
