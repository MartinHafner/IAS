{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# V2A2_Regression.py\n",
    "# Programmgeruest zu Versuch 2, Aufgabe 2\n",
    "\n",
    "import numpy as np\n",
    "import scipy.spatial\n",
    "from random import randint\n",
    "\n",
    "# ----------------------------------------------------------------------------------------- \n",
    "# base class for regressifiers\n",
    "# ----------------------------------------------------------------------------------------- \n",
    "class Regressifier:\n",
    "    \"\"\"\n",
    "    Abstract base class for regressifiers\n",
    "    Inherit from this class to implement a concrete regression algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self,X,T):        # train/compute regression with lists of feature vectors X and class labels T\n",
    "        \"\"\"\n",
    "        Train regressifier by training data X, T, should be overwritten by any derived class\n",
    "        :param X: Data matrix of size NxD, contains in each row a data vector of size D\n",
    "        :param T: Target vector matrix of size NxK, contains in each row a target vector of size K\n",
    "        :returns: -\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def predict(self,x):      # predict a target vector given the data vector x \n",
    "        \"\"\"\n",
    "        Implementation of the regression algorithm; should be overwritten by any derived class \n",
    "        :param x: test data vector of size D\n",
    "        :returns: predicted target vector\n",
    "        \"\"\"\n",
    "        return None           \n",
    "\n",
    "    def crossvalidate(self,S,X,T,dist=lambda t: np.linalg.norm(t)):  # do a S-fold cross validation \n",
    "        \"\"\"\n",
    "        Do a S-fold cross validation\n",
    "        :param S: Number of parts the data set is divided into\n",
    "        :param X: Data matrix (one data vector per row)\n",
    "        :param T: Matrix of target vectors; T[n] is target vector of X[n]\n",
    "        :param dist: a fuction dist(t) returning the length of vector t (default=Euklidean)\n",
    "        :returns (E_dist,sd_dist,E_min,E_max) : mean, standard deviation, minimum, and maximum of absolute error \n",
    "        :returns (Erel_dist,sdrel_dist,Erel_min,Erel_max) : mean, standard deviation, minimum, and maximum of relative error \n",
    "        \"\"\"\n",
    "        X,T=np.array(X),np.array(T)                         # ensure array type\n",
    "        N=len(X)                                            # N=number of data vectors\n",
    "        perm = np.random.permutation(N)                     # do a random permutation of X and T...\n",
    "        X1,T1=[X[i] for i in perm], [T[i] for i in perm]    # ... to get random partitions of the data set\n",
    "        idxS = [range(i*N//S,(i+1)*N//S) for i in range(S)] # divide data set into S parts:\n",
    "        E_dist,E_dist2,E_max,E_min=0,0,-1,-1                # initialize first two moments of (absolute) regression error as well as max/min error \n",
    "        Erel_dist,Erel_dist2,Erel_max,Erel_min=0,0,-1,-1    # initialize first two moments of relative regression error as well as max/min error \n",
    "        for idxTest in idxS:                                # loop over all possible test data sets\n",
    "            # (i) generate training and testing data sets and train classifier        \n",
    "            idxLearn = [i for i in range(N) if i not in idxTest]                      # remaining indices (not in idxTest) are learning data\n",
    "            if(S<=1): idxLearn=idxTest                                                # if S==1 use entire data set for learning and testing\n",
    "            X_learn, T_learn = np.array([X1[i] for i in idxLearn]), np.array([T1[i] for i in idxLearn]) # learn data \n",
    "            X_test , T_test  = np.array([X1[i] for i in idxTest ]), np.array([T1[i] for i in idxTest ]) # test data \n",
    "            self.fit(X_learn,T_learn)                       # train regressifier\n",
    "            # (ii) test regressifier\n",
    "            for i in range(len(X_test)):  # loop over all data vectors to be tested\n",
    "                # (ii.a) regress for i-th test vector\n",
    "                xn_test = X_test[i].T                           # data vector for testing\n",
    "                t_test = self.predict(xn_test)                  # predict target value for given test vector \n",
    "                # (ii.b) check for regression errors\n",
    "                t_true = T_test[i].T                            # true target value\n",
    "                d=dist(t_test-t_true)                           # (Euklidean) distance between t_test and t_true \n",
    "                dttrue=dist(t_true)                             # length of t_true\n",
    "                E_dist  = E_dist+d                              # sum up distances (for first moment)\n",
    "                E_dist2 = E_dist2+d*d                           # sum up squared distances (for second moment)\n",
    "                if(E_max<0)or(d>E_max): E_max=d                 # collect maximal error\n",
    "                if(E_min<0)or(d<E_min): E_min=d                 # collect minimal error\n",
    "                drel=d/dttrue\n",
    "                Erel_dist  = Erel_dist+drel                     # sum up relative distances (for first moment)\n",
    "                Erel_dist2 = Erel_dist2+(drel*drel)             # sum up squared relative distances (for second moment)\n",
    "                if(Erel_max<0)or(drel>Erel_max): Erel_max=drel  # collect maximal relative error\n",
    "                if(Erel_min<0)or(drel<Erel_min): Erel_min=drel  # collect minimal relative error\n",
    "        E_dist      = E_dist/float(N)                           # estimate of first moment (expected error)\n",
    "        E_dist2     = E_dist2/float(N)                          # estimate of second moment (expected squared error)\n",
    "        Var_dist    = E_dist2-E_dist*E_dist                     # variance of error\n",
    "        sd_dist     = np.sqrt(Var_dist)                         # standard deviation of error\n",
    "        Erel_dist   = Erel_dist/float(N)                        # estimate of first moment (expected error)\n",
    "        Erel_dist2  = Erel_dist2/float(N)                       # estimate of second moment (expected squared error)\n",
    "        Varrel_dist = Erel_dist2-Erel_dist*Erel_dist            # variance of error\n",
    "        sdrel_dist  = np.sqrt(Varrel_dist)                      # standard deviation of error\n",
    "        return (E_dist,sd_dist,E_min,E_max), (Erel_dist,sdrel_dist,Erel_min,Erel_max) # return mean, standard deviation, minimum, \n",
    "                                                                # and maximum error (for absolute and relative distances)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------- \n",
    "# DataScaler: scale data to standardize data distribution (for mean=0, standard deviation =1)  \n",
    "# -------------------------------------------------------------------------------------------- \n",
    "class DataScaler: \n",
    "    \"\"\"\n",
    "    Class for standardizing data vectors \n",
    "    Some regression methods require standardizing of data before training to avoid numerical instabilities!!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,X):               # X is data matrix, where rows are data vectors\n",
    "        \"\"\"\n",
    "        Constructor: Set parameters (mean, std,...) to standardize data matrix X\n",
    "        :param X: Data matrix of size NxD the standardization parameters (mean, std, ...) should be computed for \n",
    "        :returns: object of class DataScaler\n",
    "        \"\"\"\n",
    "        self.meanX = np.mean(X,0)       # mean values for each feature column\n",
    "        self.stdX  = np.std(X,0)        # standard deviation for each feature column \n",
    "        if isinstance(self.stdX,(list,tuple,np.ndarray)): \n",
    "            self.stdX[self.stdX==0]=1.0 # do not scale data with zero std (that is, constant features)\n",
    "        else:\n",
    "            if(self.stdX==0): self.stdX=1.0   # in case stdX is a scalar\n",
    "        self.stdXinv = 1.0/self.stdX    # inverse standard deviation\n",
    "\n",
    "\n",
    "    def scale(self,x):                  # scales data vector x to mean=0 and std=1\n",
    "        \"\"\"\n",
    "        scale data vector (or data matrix) x to mean=0 and s.d.=1 \n",
    "        :param x: data vector or data matrix  \n",
    "        :returns: scaled (standardized) data vector or data matrix \n",
    "        \"\"\"\n",
    "        return np.multiply(x-self.meanX,self.stdXinv)\n",
    "\n",
    "    def unscale(self,x):                # unscale data vector x to original distribution\n",
    "        \"\"\"\n",
    "        unscale data vector (or data matrix) x to original data ranges  \n",
    "        :param x: standardized data vector or data matrix  \n",
    "        :returns: unscaled data vector or data matrix \n",
    "        \"\"\"\n",
    "        return np.multiply(x,self.stdX)+self.meanX\n",
    "\n",
    "    def printState(self):\n",
    "        \"\"\"\n",
    "        print standardization parameters (mean value, standard deviation (std), and inverse of std)  \n",
    "        \"\"\"\n",
    "        print(\"mean=\",self.meanX, \" std=\",self.stdX, \" std_inv=\",self.stdXinv)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import *\n",
    "from functools import reduce\n",
    "# ----------------------------------------------------------------------------------------- \n",
    "# function to compute polynomial basis functions \n",
    "# ----------------------------------------------------------------------------------------- \n",
    "def phi_polynomial(x,deg=1):           # x should be list or np.array or 1xD matrix; returns an 1xM matrix \n",
    "    \"\"\"\n",
    "    polynomial basis function vector; may be used to transform a data vector x into a feature vector phi(x) having polynomial basis function components\n",
    "    :param x: data vector to be transformed into a feature vector\n",
    "    :param deg: degree of polynomial\n",
    "    :returns phi: feature vector \n",
    "    Example: phi_polynomial(x,3) returns for one-dimensional x the vector [1, x, x*x, x*x*x]\n",
    "    \"\"\"\n",
    "    x=np.array(np.mat(x))[0]           # ensure that x is a 1D array (first row of x)\n",
    "    D=len(x)\n",
    "    #assert (D==1) or ((D>1) and (deg<=3)), \"phi_polynomial(x,deg) not implemented for D=\"+str(D)+\" and deg=\"+str(deg)    # MODIFY CODE HERE FOR deg>3 !!!!\n",
    "    if(D==1):\n",
    "        phi = np.array([x[0]**i for i in range(deg+1)])\n",
    "    else:\n",
    "        phi = np.array(phi_helper(x.tolist(),deg))\n",
    "        #phi = np.array([])\n",
    "        #if(deg>=0):\n",
    "        #    phi = np.concatenate((phi,[1]))      # include degree 0 terms\n",
    "        #    if(deg>=1): \n",
    "        #        phi = np.concatenate((phi,x))    # includes degree 1 terms\n",
    "        #        if(deg>=2):\n",
    "        #            for i in range(D):\n",
    "        #                phi = np.concatenate(( phi, [x[i]*x[j] for j in range(i+1)] ))    # include degree 2 terms\n",
    "        #            if(deg>=3):\n",
    "        #                for i in range(D):\n",
    "        #                    for j in range(i+1):\n",
    "        #                        phi = np.concatenate(( phi, [x[i]*x[j]*x[k] for k in range(j+1)] ))   # include degree 3 terms\n",
    "                        # EXTEND CODE HERE FOR deg>3!!!!\n",
    "    return phi.T  # return basis function vector (=feature vector corresponding to data vector x)\n",
    "\n",
    "def phi_helper(x, deg):\n",
    "    if deg <= 0:\n",
    "        return [1]\n",
    "    else:\n",
    "        return phi_helper(x, deg-1) + [reduce(lambda a,b:a*b,combi) for combi in combinations_with_replacement(x, deg)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------------\n",
    "# Least Squares (ML) linear regression with sum of squares Regularization,\n",
    "# -----------------------------------------------------------------------------------------\n",
    "class LSRRegressifier(Regressifier):\n",
    "    \"\"\"\n",
    "    Class for Least Squares (or Maximum Likelihood) Linear Regressifier with sum of squares regularization \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,lmbda=0,phi=lambda x: phi_polynomial(x,1),flagSTD=0,eps=0.000001):\n",
    "        \"\"\"\n",
    "        Constructor of class LSRegressifier\n",
    "        :param lmbda: Regularization coefficient lambda\n",
    "        :param phi: Basis-functions used by the linear model (default linear polynomial)\n",
    "        :param flagSTD: If >0 then standardize data X and target values T (to mean 0 and s.d. 1)\n",
    "        :param eps: maximal residual value to tolerate (instead of zero) for numerically good conditioned problems\n",
    "        :returns: -\n",
    "        \"\"\"\n",
    "        self.lmbda=lmbda       # set regression parameter (default 0)\n",
    "        self.phi=phi           # set basis functions used for linear regression (default: degree 1 polynomials)\n",
    "        self.flagSTD=flagSTD;  # if flag >0 then data will be standardized, i.e., scaled for mean 0 and s.d. 1\n",
    "        self.eps=eps;          # maximal residual value to tolerate (instead of zero) for numerically good conditioned problems\n",
    "\n",
    "\n",
    "    def fit(self,X,T,lmbda=None,phi=None,flagSTD=None): # train/compute LS regression with data matrix X and target value matrix T\n",
    "        \"\"\"\n",
    "        Train regressifier (see lecture manuscript, theorem 3.11, p33) \n",
    "        :param X: Data matrix of size NxD, contains in each row a data vector of size D\n",
    "        :param T: Target vector matrix of size NxK, contains in each row a target vector of size K\n",
    "        :param lmbda: Regularization coefficient lambda\n",
    "        :param phi: Basis-functions used by the linear model (default linear polynomial)\n",
    "        :param flagSTD: If >0 then standardize data X and target values T (to mean 0 and s.d. 1)\n",
    "        :returns: flagOK: if >0 then all is ok, otherwise matrix inversion was bad conditioned (and results should not be trusted!!!) \n",
    "        \"\"\"\n",
    "        # (i) set parameters\n",
    "        if lmbda==None: lmbda=self.lmbda       # reset regularization coefficient?\n",
    "        if phi==None: phi=self.phi             # reset basis functions?\n",
    "        if flagSTD==None: flagSTD=self.flagSTD # standardize data vectors?\n",
    "        # (ii) scale data for mean=0 and s.d.=0 ?\n",
    "        if flagSTD>0:                          # if yes, then...\n",
    "            self.datascalerX=DataScaler(X)     # create datascaler for data matrix X\n",
    "            self.datascalerT=DataScaler(T)     # create datascaler for target matrix T\n",
    "            X=self.datascalerX.scale(X)        # scale all features (=columns) of data matrix X to mean=0 and s.d.=1\n",
    "            T=self.datascalerT.scale(T)        # ditto for target matrix T\n",
    "        # (iii) compute weight matrix and check numerical condition\n",
    "        flagOK,maxZ=1,0;                       # if <1 then matrix inversion is numerically infeasible\n",
    "        try:\n",
    "            self.N,self.D = X.shape            # data matrix X has size N x D (N is number of data vectors, D is dimension of a vector)\n",
    "            self.M = self.phi(self.D*[0]).size # get number of basis functions  \n",
    "            #self.K = T.shape[1]                # DELTE dummy code (just required for dummy code in predict(.): number of output dimensions\n",
    "            PHI = np.array([np.transpose(phi(x)) for x in X])                        # REPLACE dummy code: compute design matrix\n",
    "            PHIT_PHI_lmbdaI = np.add(np.dot(PHI.T,PHI),np.dot(self.lmbda,np.identity(self.M))) \n",
    "            #print(PHIT_PHI_lmbdaI)\n",
    "            # REPLACE dummy code: compute PHI_T*PHI+lambda*I\n",
    "            PHIT_PHI_lmbdaI_inv = np.linalg.inv(PHIT_PHI_lmbdaI)         # REPLACE dummy code: compute inverse matrix (may be bad conditioned and fail)\n",
    "            self.W_LSR = np.dot(np.dot(PHIT_PHI_lmbdaI_inv,PHI.T),T)# REPLACE dummy code: compute regularized least squares weights \n",
    "            # (iv) check numerical condition\n",
    "            Z=np.dot(PHIT_PHI_lmbdaI,PHIT_PHI_lmbdaI_inv)-np.identity(self.M)  # REPLACE dummy code: Compute Z:=PHIT_PHI_lmbdaI*PHIT_PHI_lmbdaI_inv-I which should become the zero matrix if good conditioned!\n",
    "            maxZ = np.max(Z)    # REPLACE dummy code: Compute maximum (absolute) componente of matrix Z (should be <eps for good conditioned problem)\n",
    "            assert maxZ<=self.eps              # maxZ should be <eps for good conditioned problems (otherwise the result cannot be trusted!!!)\n",
    "        except: \n",
    "            flagOK=0;\n",
    "            print(\"EXCEPTION DUE TO BAD CONDITION:flagOK=\", flagOK, \" maxZ=\", maxZ, \" eps=\", self.eps)\n",
    "            raise\n",
    "        return flagOK \n",
    "\n",
    "    def predict(self,x,flagSTD=None):      # predict a target value given data vector x \n",
    "        \"\"\"\n",
    "        predicts the target value y(x) for a test vector x\n",
    "        :param x: test data vector of size D\n",
    "        :param flagSTD: If >0 then standardize data X and target values T (to mean 0 and s.d. 1)\n",
    "        :returns: predicted target vector y of size K\n",
    "        \"\"\"\n",
    "        if flagSTD==None: flagSTD=self.flagSTD      # standardazion?\n",
    "        if flagSTD>0: x=self.datascalerX.scale(x)   # if yes, then scale x before computing the prediction!\n",
    "        phi_of_x = self.phi(x)                      # compute feature vector phi_of_x for data vector x\n",
    "        y=np.dot(self.W_LSR.T, phi_of_x)                    # REPLACE dummy code:  compute prediction y for data vector x \n",
    "        if flagSTD>0: y=self.datascalerT.unscale(y) # scale prediction back to original range?\n",
    "        return y                  # return prediction y for data vector x\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------------\n",
    "# KNN regression \n",
    "# -----------------------------------------------------------------------------------------\n",
    "class KNNRegressifier(Regressifier): \n",
    "    \"\"\"\n",
    "    Class for fast K-Nearest-Neighbor-Regression using KD-trees \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,K,flagKLinReg=0):\n",
    "        \"\"\"\n",
    "        Constructor of class KNNRegressifier\n",
    "        :param K: number of nearest neighbors that are used to compute prediction \n",
    "        :flagKLinReg: if >0 then the do a linear (least squares) regression on the the K nearest neighbors and their target values\n",
    "                      otherwise just take the mean of the K nearest neighbors target vectors\n",
    "        :returns: -\n",
    "        \"\"\"\n",
    "        self.K = K                                 # K is number of nearest-neighbors used for majority decision\n",
    "        self.X, self.T = [],[]                     # initially no data is stored\n",
    "        self.flagKLinReg=flagKLinReg               # if flag is set then do a linear regression of the KNN (otherwise just return mean T of the KNN)\n",
    "\n",
    "    def fit(self,X,T): # train/compute regression with lists of data vectors X and target values T\n",
    "        \"\"\"\n",
    "        Train regressifier by stroing X and T and by creating a KD-Tree based on X   \n",
    "        :param X: Data matrix of size NxD, contains in each row a data vector of size D\n",
    "        :param T: Target vector matrix of size NxK, contains in each row a target vector of size K\n",
    "        :returns: -\n",
    "        \"\"\"\n",
    "        self.X, self.T = np.array(X),np.array(T)   # just store feature vectors X and corresponding class labels T\n",
    "        self.N, self.D = self.X.shape              # store data number N and dimension D\n",
    "        self.kdtree = scipy.spatial.KDTree(self.X) # do an indexing of the feature vectors\n",
    "\n",
    "    def predict(self,x,K=None,flagKLinReg=None):   # predict a target value given data vector x \n",
    "        \"\"\"\n",
    "        predicts the target value y(x) for a test vector x\n",
    "        :param x: test data vector of size D\n",
    "        :param K: number of nearest neighbors that are used to compute prediction \n",
    "        :flagKLinReg: if >0 then the do a linear (least squares) regression on the the K nearest neighbors and their target values\n",
    "                      otherwise just take the mean of the K nearest neighbors target vectors\n",
    "        :returns: predicted target vector of size K\n",
    "        \"\"\"\n",
    "        if(K==None): K=self.K                      # do a K-NN search...\n",
    "        if(flagKLinReg==None): flagKLinReg=self.flagKLinReg # if flag >0 then do a regression on the K Nearest Neighbors to get the prediction\n",
    "        nn = self.kdtree.query(x,K)                # get indexes of K nearest neighbors of x\n",
    "        if K==1: idxNN=[nn[1]]                     # cast nearest neighbor indexes nn as a list idxNN\n",
    "        else: idxNN=nn[1]\n",
    "        t_out=0\n",
    "        if(self.flagKLinReg==0):\n",
    "            # just take mean value of KNNs\n",
    "            t_out=np.mean([self.T[i] for i in idxNN])\n",
    "        else:\n",
    "            # do a linear regression of the KNNs\n",
    "            lsr=LSRRegressifier(lmbda=0.0001,phi=lambda x:phi_polynomial(x,1),flagSTD=1)\n",
    "            lsr.fit(self.X[idxNN],self.T[idxNN])\n",
    "            t_out=lsr.predict(x)\n",
    "        return t_out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------------------\n",
      "Example: 1D-linear regression problem\n",
      "-----------------------------------------\n",
      "X= [[ 0. ]\n",
      " [ 0.5]\n",
      " [ 1. ]\n",
      " [ 1.5]\n",
      " [ 2. ]\n",
      " [ 2.5]\n",
      " [ 3. ]\n",
      " [ 3.5]\n",
      " [ 4. ]\n",
      " [ 4.5]\n",
      " [ 5. ]\n",
      " [ 5.5]\n",
      " [ 6. ]\n",
      " [ 6.5]\n",
      " [ 7. ]\n",
      " [ 7.5]\n",
      " [ 8. ]\n",
      " [ 8.5]\n",
      " [ 9. ]\n",
      " [ 9.5]\n",
      " [10. ]\n",
      " [10.5]\n",
      " [11. ]\n",
      " [11.5]\n",
      " [12. ]\n",
      " [12.5]\n",
      " [13. ]\n",
      " [13.5]\n",
      " [14. ]\n",
      " [14.5]\n",
      " [15. ]\n",
      " [15.5]\n",
      " [16. ]\n",
      " [16.5]\n",
      " [17. ]\n",
      " [17.5]\n",
      " [18. ]\n",
      " [18.5]\n",
      " [19. ]\n",
      " [19.5]\n",
      " [20. ]\n",
      " [20.5]\n",
      " [21. ]\n",
      " [21.5]\n",
      " [22. ]\n",
      " [22.5]\n",
      " [23. ]\n",
      " [23.5]\n",
      " [24. ]\n",
      " [24.5]\n",
      " [25. ]\n",
      " [25.5]\n",
      " [26. ]\n",
      " [26.5]\n",
      " [27. ]\n",
      " [27.5]\n",
      " [28. ]\n",
      " [28.5]\n",
      " [29. ]\n",
      " [29.5]\n",
      " [30. ]\n",
      " [30.5]\n",
      " [31. ]\n",
      " [31.5]\n",
      " [32. ]\n",
      " [32.5]\n",
      " [33. ]\n",
      " [33.5]\n",
      " [34. ]\n",
      " [34.5]\n",
      " [35. ]\n",
      " [35.5]\n",
      " [36. ]\n",
      " [36.5]\n",
      " [37. ]\n",
      " [37.5]\n",
      " [38. ]\n",
      " [38.5]\n",
      " [39. ]\n",
      " [39.5]\n",
      " [40. ]\n",
      " [40.5]\n",
      " [41. ]\n",
      " [41.5]\n",
      " [42. ]\n",
      " [42.5]\n",
      " [43. ]\n",
      " [43.5]\n",
      " [44. ]\n",
      " [44.5]\n",
      " [45. ]\n",
      " [45.5]\n",
      " [46. ]\n",
      " [46.5]\n",
      " [47. ]\n",
      " [47.5]\n",
      " [48. ]\n",
      " [48.5]\n",
      " [49. ]\n",
      " [49.5]]\n",
      "T= [[  3.96539552]\n",
      " [  5.40900355]\n",
      " [  6.80479406]\n",
      " [  8.03458322]\n",
      " [  8.3053615 ]\n",
      " [  8.73249338]\n",
      " [  8.77151182]\n",
      " [  9.63643954]\n",
      " [ 12.01447684]\n",
      " [ 14.10503997]\n",
      " [ 14.53639261]\n",
      " [ 16.56342681]\n",
      " [ 15.55803052]\n",
      " [ 17.54062696]\n",
      " [ 18.10718482]\n",
      " [ 20.19849118]\n",
      " [ 19.69947678]\n",
      " [ 21.30766924]\n",
      " [ 21.40690018]\n",
      " [ 22.14653271]\n",
      " [ 22.94023538]\n",
      " [ 25.36725429]\n",
      " [ 26.22817455]\n",
      " [ 27.78178612]\n",
      " [ 28.41925754]\n",
      " [ 27.71492035]\n",
      " [ 29.23188689]\n",
      " [ 29.9793288 ]\n",
      " [ 31.95354147]\n",
      " [ 32.47758882]\n",
      " [ 32.27269297]\n",
      " [ 36.15945205]\n",
      " [ 35.22589939]\n",
      " [ 38.2674975 ]\n",
      " [ 39.33027519]\n",
      " [ 38.71212506]\n",
      " [ 40.19112542]\n",
      " [ 40.37822356]\n",
      " [ 42.08476023]\n",
      " [ 41.09623498]\n",
      " [ 43.07393777]\n",
      " [ 44.58435281]\n",
      " [ 46.23278059]\n",
      " [ 46.80294075]\n",
      " [ 48.494492  ]\n",
      " [ 47.73994133]\n",
      " [ 50.94662245]\n",
      " [ 50.73035166]\n",
      " [ 50.82361583]\n",
      " [ 54.41661601]\n",
      " [ 54.32981823]\n",
      " [ 55.41985461]\n",
      " [ 54.82862886]\n",
      " [ 58.04696979]\n",
      " [ 57.30488021]\n",
      " [ 59.91427266]\n",
      " [ 60.44129026]\n",
      " [ 62.53071307]\n",
      " [ 60.97598519]\n",
      " [ 62.26103967]\n",
      " [ 64.32530732]\n",
      " [ 64.36329645]\n",
      " [ 68.68025287]\n",
      " [ 66.79291027]\n",
      " [ 68.05828692]\n",
      " [ 69.51147636]\n",
      " [ 69.78195668]\n",
      " [ 72.01666756]\n",
      " [ 72.378812  ]\n",
      " [ 73.80618286]\n",
      " [ 73.32671575]\n",
      " [ 74.58556612]\n",
      " [ 75.35734468]\n",
      " [ 77.2098026 ]\n",
      " [ 79.00187307]\n",
      " [ 79.67579371]\n",
      " [ 79.86904299]\n",
      " [ 80.98003463]\n",
      " [ 82.03965717]\n",
      " [ 82.55033359]\n",
      " [ 81.91388516]\n",
      " [ 86.1249457 ]\n",
      " [ 84.73057236]\n",
      " [ 86.42908599]\n",
      " [ 86.79082706]\n",
      " [ 88.72028873]\n",
      " [ 90.40730705]\n",
      " [ 91.78703418]\n",
      " [ 91.15956868]\n",
      " [ 93.33433493]\n",
      " [ 93.12277541]\n",
      " [ 96.2559933 ]\n",
      " [ 95.55246418]\n",
      " [ 95.90050568]\n",
      " [ 97.07683317]\n",
      " [ 98.35082574]\n",
      " [ 99.51604217]\n",
      " [100.21538238]\n",
      " [103.11867279]\n",
      " [102.75984867]]\n",
      "phi(4)= [1 4]\n",
      "phi([1,2])= [1 1 2]\n",
      "\n",
      "-----------------------------------------\n",
      "Do a Least-Squares-Regression\n",
      "-----------------------------------------\n",
      "lsr.W_LSR= [[4.09052788]\n",
      " [1.99478017]]\n",
      "prediction of x= [3.1415] is y= [10.35712978]\n",
      "LSRRegression cross-validation: absolute errors (E,sd,min,max)= (0.7210491003592423, 0.5070165487566235, 0.0066007843635347285, 2.780939336314603)   relative errors (E,sd,min,max)= (0.02240702968734219, 0.028216719179408916, 0.0001525998169486947, 0.14635559861181296)\n",
      "\n",
      "-----------------------------------------\n",
      "Do a KNN-Regression\n",
      "-----------------------------------------\n",
      "prediction of x= [3.1415] is y= 9.203975679168057\n",
      "KNNRegression cross-validation: absolute errors (E,sd,min,max)= (1.1743092886650586, 0.9170722237516987, 0.0038023455924474092, 4.2882465799825695)   relative errors (E,sd,min,max)= (0.05354319692710508, 0.13202983166413382, 5.0979643788569236e-05, 1.0603171407681475)\n"
     ]
    }
   ],
   "source": [
    "# *******************************************************\n",
    "# __main___\n",
    "# Module test\n",
    "# *******************************************************\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"\\n-----------------------------------------\")\n",
    "    print(\"Example: 1D-linear regression problem\")\n",
    "    print(\"-----------------------------------------\")\n",
    "    # (i) generate data\n",
    "    N=100\n",
    "    w0,w1=4,2                 # parameters of line\n",
    "    X=np.zeros((N,1))         # x data: allocate Nx1 matrix as numpy ndarray\n",
    "    X[:,0]=np.arange(0,50.0,50.0/N)  # equidistant sampling of the interval [0,50)\n",
    "    T=np.zeros((N,1))         # target values: allocate Nx1 matrix as numpy ndarray\n",
    "    sd_noise = 1.0            # noise power (=standard deviation)\n",
    "    T=T+w1*X+w0 + np.random.normal(0,sd_noise,T.shape)  # generate noisy target values on line y=w0+w1*x\n",
    "    par_lambda = 0            # regularization parameter\n",
    "    print(\"X=\",X)\n",
    "    print(\"T=\",T)\n",
    "\n",
    "    # (ii) define basis functions (phi should return list of basis functions; x should be a list)\n",
    "    deg=2;                               # degree of polynomial\n",
    "    phi=lambda x: phi_polynomial(x,1)    # define phi by polynomial basis-functions up to degree deg \n",
    "    print(\"phi(4)=\", phi([4]))            # print basis function vector [1, x, x*x ...] for x=4\n",
    "    print(\"phi([1,2])=\", phi([1,2]))      # print basis function vector for two-dim. inputs (yields many output components) \n",
    "\n",
    "    # (iii) compute LSR regression\n",
    "    print(\"\\n-----------------------------------------\")\n",
    "    print(\"Do a Least-Squares-Regression\")\n",
    "    print(\"-----------------------------------------\")\n",
    "    lmbda=0;\n",
    "    lsr = LSRRegressifier(lmbda,phi)\n",
    "    lsr.fit(X,T)\n",
    "    print(\"lsr.W_LSR=\",lsr.W_LSR)           # weight vector (should be approximately [w0,w1]=[4,2])\n",
    "    x=np.array([3.1415]).T\n",
    "    print(\"prediction of x=\",x,\"is y=\",lsr.predict(x))\n",
    "\n",
    "    # do S-fold crossvalidation\n",
    "    S=3\n",
    "    err_abs,err_rel = lsr.crossvalidate(S,X,T)\n",
    "    print(\"LSRRegression cross-validation: absolute errors (E,sd,min,max)=\", err_abs, \"  relative errors (E,sd,min,max)=\", err_rel)\n",
    "\n",
    "    # (iv) compute KNN-regression\n",
    "    print(\"\\n-----------------------------------------\")\n",
    "    print(\"Do a KNN-Regression\")\n",
    "    print(\"-----------------------------------------\")\n",
    "    K=2;\n",
    "    knnr = KNNRegressifier(K)\n",
    "    knnr.fit(X,T)\n",
    "    print(\"prediction of x=\",x,\"is y=\",knnr.predict(x))\n",
    "\n",
    "    # do S-fold crossvalidation\n",
    "    err_abs,err_rel = knnr.crossvalidate(S,X,T)\n",
    "    print(\"KNNRegression cross-validation: absolute errors (E,sd,min,max)=\", err_abs, \"  relative errors (E,sd,min,max)=\", err_rel) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.04137596473341779, 0.07148122842087293, 6.810069605730603e-05, 0.44728370446074633)\n",
      "(0.04137596473341779, 0.07148122842087293, 6.810069605730603e-05, 0.44728370446074633)\n",
      "(0.1297162287702424, 0.39638164915995594, 5.395317915671042e-05, 3.1265953272944778)\n",
      "(0.04137596473341779, 0.07148122842087293, 6.810069605730603e-05, 0.44728370446074633)\n",
      "(2, (1.0953802187892574, 0.8689819772601937, 0.003291802811396849, 3.884868724720441), (0.04137596473341779, 0.07148122842087293, 6.810069605730603e-05, 0.44728370446074633))\n"
     ]
    }
   ],
   "source": [
    "temp = []\n",
    "for i,e in enumerate(KNNErrors):\n",
    "    temp.append((e[2][3],i))\n",
    "temp.sort()\n",
    "#print(temp)\n",
    "\n",
    "smallest_mean= KNNErrors[1][2]\n",
    "smallest_sd= KNNErrors[1][2]\n",
    "smallest_min= KNNErrors[13][2]\n",
    "smallest_max= KNNErrors[1][2]\n",
    "print(smallest_mean)\n",
    "print(smallest_sd)\n",
    "print(smallest_min)\n",
    "print(smallest_max)\n",
    "print(KNNErrors[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Versuchen Sie zunächst den Aufbau des Moduls V2A2_Regression.py zu verstehen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Betrachten Sie den Aufbau des Moduls durch Eingabe von pydoc V2A2_Regressifier.  \n",
    "Welche Klassen gehören zu dem Modul und welchen Zweck haben sie jeweils?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DataScaler: Standatisiert Daten\n",
    "- Regressifier: Abstrakte Klasse für Regressierer\n",
    "- KNNRegressifier: ermöglicht Regression mithilfe von Fast-KNN\n",
    "- LSRRegressifier: ermöglicht Regression mithilfe von Fehlerquadratsummen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Betrachten Sie nun die Basis-Klasse Regressifier im Quelltext: Wozu dienen jeweils\n",
    "die Methoden fit(self,X,T), predict(self,x)\n",
    "und crossvalidate(self,S,X,T) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fit(self,X,T): Verlangt nach der implimentierung einer Methode die den Regressierer mit X und T trainiert.\n",
    "- predict(self,x): Verlangt nach der implimentierung einer Methode die auf x eine Regression anwendet und eine Vorhersage zurückliefert \n",
    "- crossvalidate(self,S,X,T): Macht S-fache Kreuzvaliedierung mit den Daten,Labeln (X,T) und liefert als Ergebniss Informationen über die relativen und absoluten Fehlerwerte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worin unterscheidet sich crossvalidate(.) von der entsprechenden Methode für\n",
    "Klassifikation (siehe vorigen Versuch)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hier kann man noch angeben mit welcher Längenfunktion gearbeitet werden soll, und es werden andere Fehlerstatistiken zurückgegeben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### b) Betrachten Sie nun die Funktion phi_polynomial(x,deg):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was berechnet die Funktion? Welches Ergebnis liefert phi_polynomial([3],5)?\n",
    "Welches Ergebnis liefert phi_polynomial([3,5],2)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   3,   9,  27,  81, 243])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi_polynomial([3],5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  3,  5,  9, 15, 25])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi_polynomial([3,5],2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- phi_polynomial(x,deg): berechnet die Basisfunktion für die Werte x mit dem grad deg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geben Sie eine allgemeine Formel an für das Ergebnis von phi_polynomial([x1,x2],2)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1, x1, x2, (x1)², x1*x2, (x2)²] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wozu braucht man diese Funktion im Zusammenhang mit Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Das Kreuzprodukt aus dieser Funktion und dem Zielwertevektor geben die Prognose bei der Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bis zu welchem Polynomgrad kann die Funktion Basisfunktionen berechnen?\n",
    "Erweitern Sie die Funktion mindestens bis Grad 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- momentan bis grad 3, siehe Verbesserung im code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,    3,    5,    9,   15,   25,   27,   45,   75,  125,   81,\n",
       "        135,  225,  375,  625,  243,  405,  675, 1125, 1875, 3125])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi_polynomial([3,5],5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###### c) Betrachten Sie die Klasse LSRRegressifier:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welche Art von Regressions-Modell berechnet diese Klasse?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fehlerquadratsummenregression mit Regularisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wozu dienen jeweils die Parameter lmbda, phi, flagSTD und eps ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lmbda: Regularisierungsparameter\n",
    "- phi: Basisfunktion\n",
    "- flagSTD: gibt an ob die Daten Standatisiert werden sollen\n",
    "- eps: höchster erlaubter Fehlerrestwert \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welche Rolle spielt hier die Klasse DataScaler?\n",
    "In welchen Methoden und zu welchem Zweck werden die Daten ggf. umskaliert?\n",
    "Welches Problem kann auftreten wenn man dies nicht tut?\n",
    "Wozu braucht man die Variablen Z und maxZ in der Methode fit(.)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DataScaler wird in fit() und predict() verwendet um Daten zu Standartisieren \n",
    "- nicht scalierte Daten werde können kleinste abweichungen in Gewichten zu großen und/oder unterschiedlichen Änderungen führen.\n",
    "- Z ist die Matrix mit den Fehlerwerten die durch Invertierung entstehen.\n",
    "- maxZ ist der größte Wert aus Z und wird verwendet um zu bestimmen ob der Fehlerwert annehmbar ist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vervollständigen Sie die Methoden fit(self,X,T,...) und predict(self,x,...)\n",
    "(vgl. vorige Aufgabe)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- siehe code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### d) Betrachten Sie die Klasse KNNRegressifier:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welche Art von Regressions-Modell berechnet diese Klasse?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fast K-Nearest-Neighbor-Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wozu dienen jeweils die Parameter K und flagKLinReg?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- K: Anzahl der NN die verwendet werden um eine Vorhersage zu treffen\n",
    "- flagKLinReg: gibt an ob eine Regression verwendet werden soll oder nur der Mittelwert aus den NN genommen wird"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beschreiben Sie kurz in eigenen Worten (2-3 Sätze) auf welche Weise die Prädiktion\n",
    "y(x) berechnet wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Es werden die KNNs für x ermittelt. Damit wird ein LSRRegressifier trainiert, der dann nach einer prediction gefragt wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### e) Betrachten Sie abschließend den Modultest:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beschreiben Sie kurz was im Modultest passiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- es wird ein Linearer Datensatz erstellt\n",
    "- die Funktion phi wird neu definiert und auf Grad 2 festgesetzt\n",
    "- LSR Regression wird durchgeführt und Kreuzvalidiert\n",
    "- KNN Regression wird durchgeführt und Kreuzvalidiert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welche Gewichte W werden gelernt? Wie lautet also die gelernte Prädiktionsfunktion?\n",
    "Welche Funktion sollte sich idealerweise (für N → ∞) ergeben?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "w0 = [3.78705442]  \n",
    "w1 = [2.01036841]  \n",
    "   \n",
    "y = 3.787 + 2.010*x\n",
    "\n",
    "y = 4 + 2*x  für N → ∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welche Ergebnisse liefert die Kreuzvalidierung? Was bedeuten die Werte?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - (E,sd,min,max)= (0.8335277318740052, 0.6423712068147653, 0.030450286307427632, 3.059745154084382)\n",
    " - E ist der mittlere Fehlerwert\n",
    " - sd ist die Standartabweichung der Fehler\n",
    " - min ist der kleinste Fehlerwert\n",
    " - max ist der größte Fehlerwert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vergleichen und Bewerten Sie die Ergebnisse von Least Squares Regression gegenüber der KNN-Regression (nach Optimierung der Hyper-Parameter λ, K, ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "LSR: absolute errors (E,sd,min,max)= (0.7210491003592423, 0.5070165487566235, 0.0066007843635347285, 2.780939336314603)    \n",
    "KNN: absolute errors (E,sd,min,max)= (1.1743092886650586, 0.9170722237516987, 0.0038023455924474092, 4.2882465799825695)  \n",
    "LSR: relative errors (E,sd,min,max)= (0.02240702968734219, 0.028216719179408916, 0.0001525998169486947, 0.14635559861181296)  \n",
    "KNN: relative errors (E,sd,min,max)= (0.05354319692710508, 0.13202983166413382, 5.0979643788569236e-05, 1.0603171407681475)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LSR scheint in fast allen Bereichen wesentlich bessere Ergebnisse als KNNR zu liefern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
